# 科学计算模块
import numpy as np
import pandas as pd

# 绘图模块
import matplotlib as mpl
import matplotlib.pyplot as plt

# Scikit-Learn相关模块
# 评估器类
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# 实用函数
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 数据准备
from sklearn.datasets import load_iris

# LightGBM基本原理与EFB降维方法
# 是一个基于梯度提升决策树（Gradient Boosted Decision Trees，GBDT）的高效、可扩展的机器学习算法
# LGBM算法提出的核心目的是为了解决GBDT算法框架在处理海量数据时计算效率低下的问题
# LGBM以牺牲极小的计算精度为代价，将GBDT的计算效率提升了近20倍
# 由于LGBM“选择性的牺牲精度”从另一个角度来看其实就是抑制模型过拟合，因此在很多场景下，LGBM的算法效果甚至会好于XGB。

# LGBM充分借鉴了XGB提出的一系列提升精度的优化策略，同时在此基础之上进一步提出了一系列的数据压缩和决策树建模流程的优化策略。
# 其中数据压缩方法能够让实际训练的数据量在大幅压缩的同时仍然保持较为完整的信息，
# 而决策树建模流程方面的优化，则是在XGB提出的直方图优化算法基础上进行了大幅优化


# LightGBM的数据压缩策略
# LightGBM建模过程总共会进行三方面的数据压缩：
# 先在全样本上连续变量分箱（连续变量离散化），
# 然后同时带入离散特征和离散后的连续变量进行离散特征捆绑（合并）降维，
# 最终在每次构建一颗树之前进行样本下采样。

# 连续变量的分箱就是非常简单的等宽分箱，并且具体箱体的数量可以通过超参数进行人工调节
# 离散特征的降维，则是采用了一种所谓的互斥特征捆绑（Exclusive Feature Bundling, EFB）算法
# 来源于独热编码的逆向过程，通过把互斥的特征捆绑到一起来实现降维
# 基于梯度的单边采样（Gradient-based One-Side Sampling, GOSS）的方法，和EFB类似，这种方法能够大幅压缩数据，但同时又不会导致信息的大量损失

# LightGBM决策树建模优化方法
# LGBM采用的决策树建模优化方法有两个
# 其一是直方图优化算法，这种方法本质上是通过直方图的形式更加高效简洁的表示每次分裂前后数据节点的核心信息，并且父节点和子节点也可以通过直方图减法计算直接算得，从而加速数据集分裂的计算过程:

# 其二则是leaf wise tree growth的叶子节点优先的决策树生长策略，这其实是一种树生长的模式，
# 对于其他大多数决策树算法和集成算法来说，树都是一次生长一层，也就是所谓的Level-wise tree growth（深度优先的生长策略）
# 而LGBM则允许决策树生长过程优先按照节点进行分裂，即允许决策树“有偏”的生长

# 其优势在于能够大幅提升每颗树的收敛速度，从总体来看相当于是提升了每次迭代效率；
# 而问题则在于会每棵树的计算过程会变得更加复杂，并且存在一定的过拟合风险


# 连续变量分箱
# 首先计算连续变量的取值范围，然后人工输入的max_bin超参数，进行数量为max_bin等宽度的区间划分
# 至此，就将连续变量转化为了离散变量

# XGB也会对连续变量进行分箱，但XGB的分箱是分位数分箱，而不是等宽分箱。


# 互斥特征捆绑（Exclusive Feature Bundling，EFB）
# x1和x2两个特征存在一种这样的关系——任何一条样本都不会同时在x1和x2上取值非0，
# 在EFB原理的定义中，这种关系的特征又被称作互斥特征，而互斥特征其实是可以合成一个特征的，
# 这个合成的过程并不会有任何的信息损失，而合成的过程又被称作特征捆绑。这也就是所谓的互斥特征捆绑算法。

# 放宽互斥的条件：冲突比例（conflict_rate）概念介绍
# 这个比例用于表示两个特征中冲突（即非互斥、同时取非零值）的取值占比，来衡量两个特征互斥程度。
# LGBM提供了一个名为max_conflict_rate的超参数，用于表示最大冲突比例，
# 当两个特征的冲突比例小于我们设置的最大冲突比例（max_conflict_rate）时，
# 我们就认为这两个特征并不冲突，而是互斥的，是可以进行捆绑的


# LightGBM核心技术：GOSS采样与直方图优化算法
# LGBM并不是带入全部数据进行每棵树的训练，而是采用了一种名为基于梯度的单边采样（Gradient-based One-Side Sampling，GOSS）的下采样方法

# GOSS基本原理
# 在执行优化算法的过程中，每个样本都有一个对应的梯度计算结果，
# 如果某条样本梯度绝对值较小，则说明这条样本已经被正确的分类或者预测结果和真实结果非常接近，
# 在后续的参数更新过程中，这些梯度绝对值较小的样本对参数的改进贡献较小

# GOSS的思路是将全部样本按照梯度绝对值大小进行降序排序，然后抽取梯度绝对值最大的前a%的样本，
# 然后把其他样本都视作小梯度样本，并从这些小梯度样本中随机抽取b%个样本
# 这些大梯度样本和随机抽取的小梯度样本，就构成了接下来模型训练的数据集。
# 针对小梯度样本（一边）进行抽样、保留（另一边）全部大梯度样本，也就是单边采样一词的由来


# 具体执行GOSS的时候有以下几点需要注意：
# 1.GOSS计算过程是根据梯度的绝对值大小进行样本划分和抽样，并不是样本梯度的真实值；
# 2.这里的a%可以换成更专业的超参数名称：top_rate，而小样本抽取的b%更专业的名称则是other_rate；
# 3.我们知道样本梯度是基于预测结果计算而来的（具体来说是损失函数的一阶导数）
# 而在第一棵树构建之前我们就需要进行GOSS采样，此时还没有模型预测结果，
# 梯度计算依据的是LGBM的初始预测值，和其他集成学习类似，LGBM的初始预测值也是根据损失函数的不同类型计算得到的结果；
# 4.由于每次迭代都会更新模型参数，因此每次建树之前都会重新进行抽样，而除非人为控制迭代过程（例如使用一种非常特殊的Booster API，后面会详细介绍），否则一般来说top_rate和other_rate设置好了就不会再发生变化；
# 5.一般来说top_rate越大other_rate越小，则模型过拟合风险就越大，反之则模型学习能力会被限制，而如果这两个参数同时较大，则会增加模型训练复杂度，增加模型训练时间。


# LGBM决策树生长过程与直方图优化算法（Histogram-based Algorithm）
# 直方图优化算法（Histogram-based Algorithm）
# 即通过直方图来表示数据集（及其关键信息）在决策树生长过程中的分裂即计算过程。这种表示方法能够大幅减少数据集内存占用、提升计算速度，并且方便进行直方图差分计算——子节点的直方图可以通过从父节点的直方图中减去兄弟节点的直方图来得到。
# XGB是用直方图统计样本值的累加（并对特征进行排序），而LGBM是用直方图统计数据集的一阶导数和Hessian值的累加（并对特征进行排序）

# LGBM决策树生长的增益计算公式
# 在具体的决策树生长过程中，最重要的是进行切分点的选取，这个过程中最重要的分裂增益的计算方法
# LGBM采用了一种非常特殊的、同时包含梯度和Hessian值得分裂增益计算方法
# 而对于reg_alpha和reg_lambda，其实是模型的两个超参数，也被称作L1正则化参数和L2正则化参数，用于控制模型结构风险。
# L1正则化参数和L2正则化参数取值越大，增益计算结果就越小，决策树就越倾向于不分裂。

# Leaf wise tree growth生长策略
# 其中CART树就是一种Leaf wise tree growth，而C3.0就是Level-wise tree growth，这里我们可以理解为LGBM就是采用了CART树类似的生长流程即可
# 相比层次优先的生长策略，叶节点优先的生长策略会耗费更多的计算量、但同时也会有更高的预测精度。
# 对于其他大多数集成学习算法来说，为了提高计算效率，其实都是采用的层次优先的生长策略（比如XGB）
# ，而LGBM经过了一系列数据压缩和其他优化方法，本来就拥有非常高的计算效率，
# 因此在具体决策树建模的时候采用了一种更高精度叶节点优先的策略，
# 来确保其在压缩后的数据上能够获得更高的预测精度。
# 叶节点优先的生长策略也会增加模型过拟合风险，
# 因此对于LGBM来说，必须要通过限制树的最大深度来解决叶节点优先带来的过拟合问题，
# 因此，对于LGBM来说，在超参数优化时树的最大深度max_depth将会是一个非常重要的超参数。


